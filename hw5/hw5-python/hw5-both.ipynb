{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_fileName=\"train-images.idx3-ubyte\"\n",
    "train_label_fileName=\"train-labels.idx1-ubyte\"\n",
    "test_data_fileName=\"t10k-images.idx3-ubyte\"\n",
    "test_label_fileName=\"t10k-labels.idx1-ubyte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(fileName):\n",
    "    with open(fileName,'rb') as obj:\n",
    "        data=obj.read(16)\n",
    "        magic,N,n_row,n_col=struct.unpack('!4i',data)\n",
    "        images=np.zeros((N,n_row,n_col))\n",
    "        for i in range(N):\n",
    "            data=obj.read(n_row*n_col)\n",
    "            images[i]=np.array(struct.unpack('!'+str(n_row*n_col)+'B',data)).reshape((n_row,n_col))\n",
    "    return images\n",
    "def readLabels(fileName):\n",
    "    with open(fileName,'rb') as obj:\n",
    "        data=obj.read(8)\n",
    "        magic,N=struct.unpack('!2i',data)\n",
    "        data=obj.read(N)\n",
    "        labels=np.array(struct.unpack('!'+str(N)+'B',data))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=readData(train_data_fileName)\n",
    "N,n_row,n_col=train_data.shape\n",
    "train_data=train_data.reshape((N,n_row,n_col,1))\n",
    "train_labels=np.eye(10)[readLabels(train_label_fileName)]\n",
    "\n",
    "test_data=readData(test_data_fileName)\n",
    "N,n_row,n_col=test_data.shape\n",
    "test_data=test_data.reshape((N,n_row,n_col,1))\n",
    "test_labels=np.eye(10)[readLabels(test_label_fileName)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data=np.concatenate([train_data,255-train_data],axis=0)\n",
    "train_labels=np.concatenate([train_labels,train_labels],axis=0)\n",
    "\n",
    "test_data=np.concatenate([test_data,255-test_data],axis=0)\n",
    "test_labels=np.concatenate([test_labels,test_labels],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(input_size=(28,28,1)):\n",
    "    initial_method=\"random_uniform\"\n",
    "    X_Input=keras.layers.Input(input_size)\n",
    "    X=keras.layers.Conv2D(6,kernel_size=(5,5),strides=(1,1),activation='relu',name='C1',padding='same',kernel_initializer=initial_method)(X_Input)\n",
    "    X=keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2),name='S2')(X)\n",
    "    X=keras.layers.Conv2D(16,kernel_size=(5,5),strides=(1,1),activation='relu',name='C3',kernel_initializer=initial_method)(X)\n",
    "    X=keras.layers.MaxPool2D(pool_size=(2,2),strides=(2,2),name='S4')(X)\n",
    "    X=keras.layers.Flatten(name='flatten')(X)\n",
    "    X=keras.layers.Dense(120,activation='relu',name='C5',kernel_initializer=initial_method)(X)\n",
    "    X=keras.layers.Dense(84,activation='relu',name='C6',kernel_initializer=initial_method)(X)\n",
    "    X=keras.layers.Dense(10,activation='softmax',name='Output',kernel_initializer=initial_method)(X)\n",
    "    model=keras.models.Model(inputs=X_Input,outputs=X,name='myModel')\n",
    "    return model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model=Model()\n",
    "model.compile(metrics=['accuracy'],optimizer='adam',loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 120000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "120000/120000 [==============================] - 9s 77us/step - loss: 0.1762 - acc: 0.9436 - val_loss: 0.0679 - val_acc: 0.9771\n",
      "\n",
      "Epoch 00001: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 2/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0553 - acc: 0.9827 - val_loss: 0.0425 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00002: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 3/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0430 - acc: 0.9868 - val_loss: 0.0387 - val_acc: 0.9877\n",
      "\n",
      "Epoch 00003: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 4/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0364 - acc: 0.9888 - val_loss: 0.0559 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00004: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 5/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0319 - acc: 0.9901 - val_loss: 0.0449 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00005: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 6/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0271 - acc: 0.9914 - val_loss: 0.0470 - val_acc: 0.9861\n",
      "\n",
      "Epoch 00006: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 7/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0260 - acc: 0.9918 - val_loss: 0.0426 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00007: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 8/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0229 - acc: 0.9927 - val_loss: 0.0487 - val_acc: 0.9869\n",
      "\n",
      "Epoch 00008: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 9/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0215 - acc: 0.9931 - val_loss: 0.0479 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00009: saving model to mnist_adam_uniform_2.h5\n",
      "Epoch 10/10\n",
      "120000/120000 [==============================] - 7s 60us/step - loss: 0.0209 - acc: 0.9935 - val_loss: 0.0509 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00010: saving model to mnist_adam_uniform_2.h5\n"
     ]
    }
   ],
   "source": [
    "optimizer_name='adam_uniform_2'\n",
    "checkpoint_best = keras.callbacks.ModelCheckpoint(\"mnist_\"+optimizer_name+\".h5\", monitor='val_acc',verbose=1,save_best_only=False)\n",
    "\n",
    "hist=model.fit(train_data,train_labels,\n",
    "         batch_size=128,\n",
    "         epochs=10,\n",
    "         shuffle=True,\n",
    "         validation_data=(test_data,test_labels),\n",
    "         callbacks=[checkpoint_best])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "fileName=optimizer_name+\"_hist.h5\"\n",
    "with h5py.File(fileName) as obj:\n",
    "    obj.create_dataset('val_loss',data=hist.history['val_loss'])\n",
    "    obj.create_dataset('val_acc',data=hist.history['val_acc'])\n",
    "    obj.create_dataset('loss',data=hist.history['loss'])\n",
    "    obj.create_dataset('acc',data=hist.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10300788 0.09113237 0.06137469 0.05830885 0.06718337 0.04471642\n",
      " 0.05229546 0.04317136 0.04272618 0.04586303]\n"
     ]
    }
   ],
   "source": [
    "fileName=\"sgd_hist.h5\"\n",
    "with h5py.File(fileName,'r') as obj:\n",
    "    print(obj['val_loss'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 6 7 ... 7 7 6]\n",
      "[7 2 1 ... 4 5 6]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "negative_test_data=255-test_data\n",
    "negative_predict=model.predict(negative_test_data)\n",
    "print(np.argmax(negative_predict,axis=1))\n",
    "print(np.argmax(test_labels,axis=1))\n",
    "right=np.argmax(negative_predict,axis=1)==np.argmax(test_labels,axis=1)\n",
    "acc=len(right)/len(test_labels)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
